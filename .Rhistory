library(doSNOW)
registerDoSNOW(makeCluster(2, type = "SOCK"))
library(reshape2)
library(doSNOW)
library(foreach)
library(doParallel)
cl = 2
registerDoSNOW(makeCluster(2, type = "SOCK"))
# parameters
data.dir    <- 'C:\\Users\\jcotter\\JPC3\\My Projects\\Kaggle\\Facial Keypoints Detection\\'
patch_size  <- 10
search_size <- 2
# read data and convert image strings to arrays
train.file <- paste0(data.dir, 'training.csv')
test.file  <- paste0(data.dir, 'test.csv')
data.file  <- paste0(data.dir, 'data.Rd')
d.train    <- read.csv(train.file, stringsAsFactors=F)
d.test     <- read.csv(test.file,  stringsAsFactors=F)
im.train   <- foreach(im = d.train$Image, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.test    <- foreach(im = d.test$Image, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
d.train$Image <- NULL
d.test$Image  <- NULL
# list the coordinates we have to predict
coordinate.names <- gsub("_x", "", names(d.train)[grep("_x", names(d.train))])
# for each one, compute the average patch
mean.patches <- foreach(coord = coordinate.names) %dopar% {
cat(sprintf("computing mean patch for %s\n", coord))
coord_x <- paste(coord, "x", sep="_")
coord_y <- paste(coord, "y", sep="_")
# compute average patch
patches <- foreach (i = 1:nrow(d.train), .combine=rbind) %do% {
im  <- matrix(data = im.train[i,], nrow=96, ncol=96)
x   <- d.train[i, coord_x]
y   <- d.train[i, coord_y]
x1  <- (x-patch_size)
x2  <- (x+patch_size)
y1  <- (y-patch_size)
y2  <- (y+patch_size)
if ( (!is.na(x)) && (!is.na(y)) && (x1>=1) && (x2<=96) && (y1>=1) && (y2<=96) )
{
as.vector(im[x1:x2, y1:y2])
}
else
{
NULL
}
}
matrix(data = colMeans(patches), nrow=2*patch_size+1, ncol=2*patch_size+1)
}
# for each coordinate and for each test image, find the position that best correlates with the average patch
p <- foreach(coord_i = 1:length(coordinate.names), .combine=cbind) %dopar% {
# the coordinates we want to predict
coord   <- coordinate.names[coord_i]
coord_x <- paste(coord, "x", sep="_")
coord_y <- paste(coord, "y", sep="_")
# the average of them in the training set (our starting point)
mean_x  <- mean(d.train[, coord_x], na.rm=T)
mean_y  <- mean(d.train[, coord_y], na.rm=T)
# search space: 'search_size' pixels centered on the average coordinates
x1 <- as.integer(mean_x)-search_size
x2 <- as.integer(mean_x)+search_size
y1 <- as.integer(mean_y)-search_size
y2 <- as.integer(mean_y)+search_size
# ensure we only consider patches completely inside the image
x1 <- ifelse(x1-patch_size<1,  patch_size+1,  x1)
y1 <- ifelse(y1-patch_size<1,  patch_size+1,  y1)
x2 <- ifelse(x2+patch_size>96, 96-patch_size, x2)
y2 <- ifelse(y2+patch_size>96, 96-patch_size, y2)
# build a list of all positions to be tested
params <- expand.grid(x = x1:x2, y = y1:y2)
# for each image...
r <- foreach(i = 1:nrow(d.test), .combine=rbind) %do% {
if ((coord_i==1)&&((i %% 100)==0)) { cat(sprintf("%d/%d\n", i, nrow(d.test))) }
im <- matrix(data = im.test[i,], nrow=96, ncol=96)
# ... compute a score for each position ...
r  <- foreach(j = 1:nrow(params), .combine=rbind) %do% {
x     <- params$x[j]
y     <- params$y[j]
p     <- im[(x-patch_size):(x+patch_size), (y-patch_size):(y+patch_size)]
score <- cor(as.vector(p), as.vector(mean.patches[[coord_i]]))
score <- ifelse(is.na(score), 0, score)
data.frame(x, y, score)
}
# ... and return the best
best <- r[which.max(r$score), c("x", "y")]
}
names(r) <- c(coord_x, coord_y)
r
}
library(reshape2)
library(doSNOW)
library(foreach)
library(doParallel)
cl = 2
# registerDoSNOW(cl)
registerDoSNOW(makeCluster(2, type = "SOCK"))
# parameters
data.dir    <- 'C:\\Users\\jcotter\\JPC3\\My Projects\\Kaggle\\Facial Keypoints Detection\\'
patch_size  <- 10
search_size <- 2
# read data and convert image strings to arrays
train.file <- paste0(data.dir, 'training.csv')
test.file  <- paste0(data.dir, 'test.csv')
data.file  <- paste0(data.dir, 'data.Rd')
d.train    <- read.csv(train.file, stringsAsFactors=F)
d.test     <- read.csv(test.file,  stringsAsFactors=F)
im.train   <- foreach(im = d.train$Image, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.test    <- foreach(im = d.test$Image, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
d.train$Image <- NULL
d.test$Image  <- NULL
# list the coordinates we have to predict
coordinate.names <- gsub("_x", "", names(d.train)[grep("_x", names(d.train))])
# for each one, compute the average patch
mean.patches <- foreach(coord = coordinate.names) %dopar% {
cat(sprintf("computing mean patch for %s\n", coord))
coord_x <- paste(coord, "x", sep="_")
coord_y <- paste(coord, "y", sep="_")
# compute average patch
patches <- foreach (i = 1:nrow(d.train), .combine=rbind) %do% {
im  <- matrix(data = im.train[i,], nrow=96, ncol=96)
x   <- d.train[i, coord_x]
y   <- d.train[i, coord_y]
x1  <- (x-patch_size)
x2  <- (x+patch_size)
y1  <- (y-patch_size)
y2  <- (y+patch_size)
if ( (!is.na(x)) && (!is.na(y)) && (x1>=1) && (x2<=96) && (y1>=1) && (y2<=96) )
{
as.vector(im[x1:x2, y1:y2])
}
else
{
NULL
}
}
matrix(data = colMeans(patches), nrow=2*patch_size+1, ncol=2*patch_size+1)
}
# for each coordinate and for each test image, find the position that best correlates with the average patch
p <- foreach(coord_i = 1:length(coordinate.names), .combine=cbind) %dopar% {
# the coordinates we want to predict
coord   <- coordinate.names[coord_i]
coord_x <- paste(coord, "x", sep="_")
coord_y <- paste(coord, "y", sep="_")
# the average of them in the training set (our starting point)
mean_x  <- mean(d.train[, coord_x], na.rm=T)
mean_y  <- mean(d.train[, coord_y], na.rm=T)
# search space: 'search_size' pixels centered on the average coordinates
x1 <- as.integer(mean_x)-search_size
x2 <- as.integer(mean_x)+search_size
y1 <- as.integer(mean_y)-search_size
y2 <- as.integer(mean_y)+search_size
# ensure we only consider patches completely inside the image
x1 <- ifelse(x1-patch_size<1,  patch_size+1,  x1)
y1 <- ifelse(y1-patch_size<1,  patch_size+1,  y1)
x2 <- ifelse(x2+patch_size>96, 96-patch_size, x2)
y2 <- ifelse(y2+patch_size>96, 96-patch_size, y2)
# build a list of all positions to be tested
params <- expand.grid(x = x1:x2, y = y1:y2)
# for each image...
r <- foreach(i = 1:nrow(d.test), .combine=rbind) %do% {
if ((coord_i==1)&&((i %% 100)==0)) { cat(sprintf("%d/%d\n", i, nrow(d.test))) }
im <- matrix(data = im.test[i,], nrow=96, ncol=96)
# ... compute a score for each position ...
r  <- foreach(j = 1:nrow(params), .combine=rbind) %do% {
x     <- params$x[j]
y     <- params$y[j]
p     <- im[(x-patch_size):(x+patch_size), (y-patch_size):(y+patch_size)]
score <- cor(as.vector(p), as.vector(mean.patches[[coord_i]]))
score <- ifelse(is.na(score), 0, score)
data.frame(x, y, score)
}
# ... and return the best
best <- r[which.max(r$score), c("x", "y")]
}
names(r) <- c(coord_x, coord_y)
r
}
# prepare file for submission
predictions        <- data.frame(ImageId = 1:nrow(d.test), p)
submission         <- melt(predictions, id.vars="ImageId", variable.name="FeatureName", value.name="Location")
example.submission <- read.csv(paste0(data.dir, 'submissionFileFormat.csv'))
sub.col.names      <- names(example.submission)
example.submission$Location <- NULL
submission <- merge(example.submission, submission, all.x=T, sort=F)
submission <- submission[, sub.col.names]
write.csv(submission, file="submission_search.csv", quote=F, row.names=F)
install.packages("bigmemory")
install.packages("biganalytics")
install.packages('bigmemory')
library(reshape2)
library(doSNOW)
library(foreach)
library(doParallel)
cl = 2
# registerDoSNOW(cl)
registerDoSNOW(makeCluster(2, type = "SOCK"))
# parameters
data.dir    <- 'C:\\Users\\jcotter\\JPC3\\My Projects\\Kaggle\\Facial Keypoints Detection\\'
patch_size  <- 10
search_size <- 2
# read data and convert image strings to arrays
train.file <- paste0(data.dir, 'training.csv')
test.file  <- paste0(data.dir, 'test.csv')
data.file  <- paste0(data.dir, 'data.Rd')
d.train    <- read.csv(train.file, stringsAsFactors=F)
d.test     <- read.csv(test.file,  stringsAsFactors=F)
im.train   <- foreach(im = d.train$Image, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.test    <- foreach(im = d.test$Image, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
d.train$Image <- NULL
d.test$Image  <- NULL
# list the coordinates we have to predict
coordinate.names <- gsub("_x", "", names(d.train)[grep("_x", names(d.train))])
# for each one, compute the average patch
mean.patches <- foreach(coord = coordinate.names) %dopar% {
cat(sprintf("computing mean patch for %s\n", coord))
coord_x <- paste(coord, "x", sep="_")
coord_y <- paste(coord, "y", sep="_")
# compute average patch
patches <- foreach (i = 1:nrow(d.train), .combine=rbind) %do% {
im  <- matrix(data = im.train[i,], nrow=96, ncol=96)
x   <- d.train[i, coord_x]
y   <- d.train[i, coord_y]
x1  <- (x-patch_size)
x2  <- (x+patch_size)
y1  <- (y-patch_size)
y2  <- (y+patch_size)
if ( (!is.na(x)) && (!is.na(y)) && (x1>=1) && (x2<=96) && (y1>=1) && (y2<=96) )
{
as.vector(im[x1:x2, y1:y2])
}
else
{
NULL
}
}
matrix(data = colMeans(patches), nrow=2*patch_size+1, ncol=2*patch_size+1)
}
library(reshape2)
library(doSNOW)
#library(foreach)
#library(doParallel)
cl = 2
# registerDoSNOW(cl)
registerDoSNOW(makeCluster(2, type = "SOCK"))
# parameters
data.dir    <- 'C:\\Users\\jcotter\\JPC3\\My Projects\\Kaggle\\Facial Keypoints Detection\\'
patch_size  <- 10
search_size <- 2
# read data and convert image strings to arrays
train.file <- paste0(data.dir, 'training.csv')
test.file  <- paste0(data.dir, 'test.csv')
data.file  <- paste0(data.dir, 'data.Rd')
d.train    <- read.csv(train.file, stringsAsFactors=F)
d.test     <- read.csv(test.file,  stringsAsFactors=F)
im.train   <- foreach(im = d.train$Image, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.test    <- foreach(im = d.test$Image, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
d.train$Image <- NULL
d.test$Image  <- NULL
# list the coordinates we have to predict
coordinate.names <- gsub("_x", "", names(d.train)[grep("_x", names(d.train))])
# for each one, compute the average patch
mean.patches <- foreach(coord = coordinate.names) %dopar% {
cat(sprintf("computing mean patch for %s\n", coord))
coord_x <- paste(coord, "x", sep="_")
coord_y <- paste(coord, "y", sep="_")
# compute average patch
patches <- foreach (i = 1:nrow(d.train), .combine=rbind) %do% {
im  <- matrix(data = im.train[i,], nrow=96, ncol=96)
x   <- d.train[i, coord_x]
y   <- d.train[i, coord_y]
x1  <- (x-patch_size)
x2  <- (x+patch_size)
y1  <- (y-patch_size)
y2  <- (y+patch_size)
if ( (!is.na(x)) && (!is.na(y)) && (x1>=1) && (x2<=96) && (y1>=1) && (y2<=96) )
{
as.vector(im[x1:x2, y1:y2])
}
else
{
NULL
}
}
matrix(data = colMeans(patches), nrow=2*patch_size+1, ncol=2*patch_size+1)
}
registerDoParallel
registerDoParallel()
# for each one, compute the average patch
mean.patches <- foreach(coord = coordinate.names) %dopar% {
cat(sprintf("computing mean patch for %s\n", coord))
coord_x <- paste(coord, "x", sep="_")
coord_y <- paste(coord, "y", sep="_")
# compute average patch
patches <- foreach (i = 1:nrow(d.train), .combine=rbind) %do% {
im  <- matrix(data = im.train[i,], nrow=96, ncol=96)
x   <- d.train[i, coord_x]
y   <- d.train[i, coord_y]
x1  <- (x-patch_size)
x2  <- (x+patch_size)
y1  <- (y-patch_size)
y2  <- (y+patch_size)
if ( (!is.na(x)) && (!is.na(y)) && (x1>=1) && (x2<=96) && (y1>=1) && (y2<=96) )
{
as.vector(im[x1:x2, y1:y2])
}
else
{
NULL
}
}
matrix(data = colMeans(patches), nrow=2*patch_size+1, ncol=2*patch_size+1)
}
# for each one, compute the average patch
mean.patches <- foreach(coord = coordinate.names) %dopar% {
cat(sprintf("computing mean patch for %s\n", coord))
coord_x <- paste(coord, "x", sep="_")
coord_y <- paste(coord, "y", sep="_")
# compute average patch
patches <- foreach (i = 1:nrow(d.train), .combine=rbind) %do% {
im  <- matrix(data = im.train[i,], nrow=96, ncol=96)
x   <- d.train[i, coord_x]
y   <- d.train[i, coord_y]
x1  <- (x-patch_size)
x2  <- (x+patch_size)
y1  <- (y-patch_size)
y2  <- (y+patch_size)
if ( (!is.na(x)) && (!is.na(y)) && (x1>=1) && (x2<=96) && (y1>=1) && (y2<=96) )
{
as.vector(im[x1:x2, y1:y2])
}
else
{
NULL
}
}
matrix(data = colMeans(patches), nrow=2*patch_size+1, ncol=2*patch_size+1)
}
library(reshape2)
library(doSNOW)
#library(foreach)
#library(doParallel)
cl = 1
# registerDoSNOW(cl)
registerDoSNOW(makeCluster(2, type = "SOCK"))
# parameters
data.dir    <- 'C:\\Users\\jcotter\\JPC3\\My Projects\\Kaggle\\Facial Keypoints Detection\\'
patch_size  <- 10
search_size <- 2
# read data and convert image strings to arrays
train.file <- paste0(data.dir, 'training.csv')
test.file  <- paste0(data.dir, 'test.csv')
data.file  <- paste0(data.dir, 'data.Rd')
d.train    <- read.csv(train.file, stringsAsFactors=F)
d.test     <- read.csv(test.file,  stringsAsFactors=F)
im.train   <- foreach(im = d.train$Image, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
im.test    <- foreach(im = d.test$Image, .combine=rbind) %dopar% {
as.integer(unlist(strsplit(im, " ")))
}
d.train$Image <- NULL
d.test$Image  <- NULL
# list the coordinates we have to predict
coordinate.names <- gsub("_x", "", names(d.train)[grep("_x", names(d.train))])
# for each one, compute the average patch
mean.patches <- foreach(coord = coordinate.names) %dopar% {
cat(sprintf("computing mean patch for %s\n", coord))
coord_x <- paste(coord, "x", sep="_")
coord_y <- paste(coord, "y", sep="_")
# compute average patch
patches <- foreach (i = 1:nrow(d.train), .combine=rbind) %do% {
im  <- matrix(data = im.train[i,], nrow=96, ncol=96)
x   <- d.train[i, coord_x]
y   <- d.train[i, coord_y]
x1  <- (x-patch_size)
x2  <- (x+patch_size)
y1  <- (y-patch_size)
y2  <- (y+patch_size)
if ( (!is.na(x)) && (!is.na(y)) && (x1>=1) && (x2<=96) && (y1>=1) && (y2<=96) )
{
as.vector(im[x1:x2, y1:y2])
}
else
{
NULL
}
}
matrix(data = colMeans(patches), nrow=2*patch_size+1, ncol=2*patch_size+1)
}
setwd("C:/Users/jcotter/JPC3/School/Data Science Specialization - Johns Hopkins/Practical Machine Learning/Course Project Github")
library(slidify)
author("Practical Machine Learning Course Project")
fullCM
# Load Required Librarys and set a seed for repeatability
library(caret)
library(randomForest)
set.seed(27272)
# Import Data Sets
training=read.csv(file="pml-training.csv",head=TRUE,sep=",",na.strings=c("NA","#DIV/0!",""))
testing=read.csv(file="pml-testing.csv",head=TRUE,sep=",")
# Clean Data by dropping first 7 columns and Removing columns with all NAs and zeros
training<-training[,-seq(1:7)]
testing<-testing[,-seq(1:7)]
hasNA<-as.vector(sapply(training[,1:152],function(x) {length(which(is.na(x)))!=0}))
training<-training[,!hasNA]
testing<-testing[,!hasNA]
# divide training data as training (70%) and testing (30%)
# I will be testing how much accuracy is lost using PCA
inTrain<-createDataPartition(training$classe, p = 0.7)[[1]]
Traintraining<-training[inTrain,]
Traintesting<-training[-inTrain,]
# preprocess with PCA for both training and testing
preProc<-preProcess(Traintraining[,-53],method="pca")
trainPCA<-predict(preProc,Traintraining[,-53])
trainPCA$classe=Traintraining$classe
testPCA<-predict(preProc,Traintesting[,-53])
testPCA$classe=Traintesting$classe
# Train with Random Forests
# Full data
fitFullRF<-randomForest(Traintraining$classe ~.,data = Traintraining,importance = TRUE)
predictFullRF<-predict(fitFullRF,Traintesting)
fullCM<-confusionMatrix(predictFullRF,Traintesting$classe)
# PCA data
fitpcaRF<-randomForest(trainPCA$classe ~.,data = trainPCA,importance = TRUE)
predictpcaRF<-predict(fitpcaRF,testPCA)
pcaCM <- confusionMatrix(predictpcaRF,testPCA$classe)
# Differences in Accuracy
fullCM$overall[1]-pcaCM$overall[1]
# PCA only loses ~1.8% Accuracy but I want to use the full data anyway
finalRF<-randomForest(training$classe ~.,data = training,importance = TRUE)
Answer<-predict(finalRF,testing)
Answer
setwd("C:/Users/jcotter/JPC3/School/Data Science Specialization - Johns Hopkins/Practical Machine Learning/Course Project Github")
# Load Required Librarys and set a seed for repeatability
library(caret)
library(randomForest)
set.seed(27272)
# Import Data Sets
training=read.csv(file="pml-training.csv",head=TRUE,sep=",",na.strings=c("NA","#DIV/0!",""))
testing=read.csv(file="pml-testing.csv",head=TRUE,sep=",")
# Clean Data by dropping first 7 columns and Removing columns with all NAs and zeros
training<-training[,-seq(1:7)]
testing<-testing[,-seq(1:7)]
hasNA<-as.vector(sapply(training[,1:152],function(x) {length(which(is.na(x)))!=0}))
training<-training[,!hasNA]
testing<-testing[,!hasNA]
# divide training data as training (70%) and testing (30%)
# I will be testing how much accuracy is lost using PCA
inTrain<-createDataPartition(training$classe, p = 0.7)[[1]]
Traintraining<-training[inTrain,]
Traintesting<-training[-inTrain,]
# preprocess with PCA for both training and testing
preProc<-preProcess(Traintraining[,-53],method="pca")
trainPCA<-predict(preProc,Traintraining[,-53])
trainPCA$classe=Traintraining$classe
testPCA<-predict(preProc,Traintesting[,-53])
testPCA$classe=Traintesting$classe
# Train with Random Forests
# Full data
fitFullRF<-randomForest(Traintraining$classe ~.,data = Traintraining,importance = TRUE)
predictFullRF<-predict(fitFullRF,Traintesting)
fullCM<-confusionMatrix(predictFullRF,Traintesting$classe)
# PCA data
fitpcaRF<-randomForest(trainPCA$classe ~.,data = trainPCA,importance = TRUE)
predictpcaRF<-predict(fitpcaRF,testPCA)
pcaCM <- confusionMatrix(predictpcaRF,testPCA$classe)
# Differences in Accuracy
fullCM$overall[1]-pcaCM$overall[1]
# PCA only loses ~1.8% Accuracy but I want to use the full data anyway
finalRF<-randomForest(training$classe ~.,data = training,importance = TRUE)
Answer<-predict(finalRF,testing)
Answer
fullCM
fullCM$overall
rfcv(fitFullRF, Traintraining$Classe)
rfcv(Traintraining, Traintraining$Classe)
rfcv(training, training$Classe)
rf.cv(training, training$Classe, cvfold=10)
rfcv(training, training$Classe, cvfold=10)
rfcv(training[,-52], training$Classe, cvfold=10)
rfcv(training[-52,], training$Classe)
rfcv(training[,-52], training$Classe)
```{r fig.height=4}
slidify("Pracitical Machine Learning Course Project")
slidify()
slidify(Practical Machine Learning Course Project)
slidify("Practical Machine Learning Course Project")
setwd("C:/Users/jcotter/JPC3/School/Data Science Specialization - Johns Hopkins/Practical Machine Learning/Course Project Github")
slidify("Practical Machine Learning Course Project")
publish(user = "johncotter3", repo = "Practical Machine Learning")
slidify("Practical Machine Learning Course Project")
slidify()
slidify("Practical Machine Learning Course Project")
slidify("Practical Machine Learning Course Project")
setwd("C:/Users/jcotter/JPC3/School/Data Science Specialization - Johns Hopkins/Practical Machine Learning/Course Project Github/Practical Machine Learning Course Project")
slidify("Practical Machine Learning Course Project")
